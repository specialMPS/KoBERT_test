{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17178,"status":"ok","timestamp":1668157391814,"user":{"displayName":"MPS special","userId":"14769746348235507214"},"user_tz":-540},"id":"67OCuTUVp9DO","outputId":"6108fa34-54f0-4242-ed57-47c164c9b165"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n"]}],"source":["# google drive mount\n","\n","from google.colab import drive\n","drive.mount('/gdrive', force_remount=True)"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"5WHzQ5MCjuhW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668157503604,"user_tz":-540,"elapsed":109580,"user":{"displayName":"MPS special","userId":"14769746348235507214"}},"outputId":"05c2ed07-9969-4d9e-e80f-50652aa15122"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting gluonnlp==0.10.0\n","  Downloading gluonnlp-0.10.0.tar.gz (344 kB)\n","\u001b[K     |████████████████████████████████| 344 kB 5.0 MB/s \n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.1)\n","Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from gluonnlp==0.10.0) (1.21.6)\n","Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from gluonnlp==0.10.0) (0.29.32)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gluonnlp==0.10.0) (21.3)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.6)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->gluonnlp==0.10.0) (3.0.9)\n","Building wheels for collected packages: gluonnlp\n","  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp37-cp37m-linux_x86_64.whl size=595753 sha256=bc160bb0e6c43527a484ffb665240b26da31ed2613df3a9c1f2529eb6b3b8c0a\n","  Stored in directory: /root/.cache/pip/wheels/be/b4/06/7f3fdfaf707e6b5e98b79c041e023acffbe395d78a527eae00\n","Successfully built gluonnlp\n","Installing collected packages: gluonnlp\n","Successfully installed gluonnlp-0.10.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting mxnet\n","  Downloading mxnet-1.9.1-py3-none-manylinux2014_x86_64.whl (49.1 MB)\n","\u001b[K     |████████████████████████████████| 49.1 MB 103.0 MB/s \n","\u001b[?25hRequirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.7/dist-packages (from mxnet) (1.21.6)\n","Collecting graphviz<0.9.0,>=0.8.1\n","  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet) (2.23.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (2022.9.24)\n","Installing collected packages: graphviz, mxnet\n","  Attempting uninstall: graphviz\n","    Found existing installation: graphviz 0.10.1\n","    Uninstalling graphviz-0.10.1:\n","      Successfully uninstalled graphviz-0.10.1\n","Successfully installed graphviz-0.8.4 mxnet-1.9.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece==0.1.91\n","  Downloading sentencepiece-0.1.91-cp37-cp37m-manylinux1_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 4.7 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.91\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers==4.8.2\n","  Downloading transformers-4.8.2-py3-none-any.whl (2.5 MB)\n","\u001b[K     |████████████████████████████████| 2.5 MB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2) (4.13.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2) (21.3)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[K     |████████████████████████████████| 880 kB 62.2 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2) (2022.6.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2) (1.21.6)\n","Collecting huggingface-hub==0.0.12\n","  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 55.4 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2) (4.64.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2) (3.8.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2) (6.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers==4.8.2) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.8.2) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.8.2) (3.10.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.8.2) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.8.2) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.8.2) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.8.2) (2022.9.24)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.8.2) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.8.2) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.8.2) (1.2.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895259 sha256=ff8761dbf17596397508f2da50433d78694df36eb047a66bddfb3a3ebe1029d3\n","  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.0.12 sacremoses-0.0.53 tokenizers-0.10.3 transformers-4.8.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torch==1.8.1\n","  Downloading torch-1.8.1-cp37-cp37m-manylinux1_x86_64.whl (804.1 MB)\n","\u001b[K     |████████████████████████████████| 804.1 MB 2.5 kB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1) (4.1.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1) (1.21.6)\n","Installing collected packages: torch\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.12.1+cu113\n","    Uninstalling torch-1.12.1+cu113:\n","      Successfully uninstalled torch-1.12.1+cu113\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 1.8.1 which is incompatible.\n","torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.8.1 which is incompatible.\n","torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.8.1 which is incompatible.\u001b[0m\n","Successfully installed torch-1.8.1\n"]}],"source":["!pip install gluonnlp==0.10.0 pandas tqdm   \n","!pip install mxnet\n","!pip install sentencepiece==0.1.91\n","!pip install transformers==4.8.2\n","!pip install torch==1.8.1"]},{"cell_type":"code","source":["!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"],"metadata":{"id":"5hiCoaNPneB6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668157632651,"user_tz":-540,"elapsed":15846,"user":{"displayName":"MPS special","userId":"14769746348235507214"}},"outputId":"5c38e331-a114-4928-d48a-64bb756c4320"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n","  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-hr3gi70a\n","  Running command git clone -q 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-hr3gi70a\n","Collecting boto3<=1.15.18\n","  Downloading boto3-1.15.18-py2.py3-none-any.whl (129 kB)\n","\u001b[K     |████████████████████████████████| 129 kB 5.0 MB/s \n","\u001b[?25hRequirement already satisfied: gluonnlp<=0.10.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from kobert==0.2.3) (0.10.0)\n","Collecting mxnet<=1.7.0.post2,>=1.4.0\n","  Downloading mxnet-1.7.0.post2-py2.py3-none-manylinux2014_x86_64.whl (54.7 MB)\n","\u001b[K     |████████████████████████████████| 54.7 MB 1.3 MB/s \n","\u001b[?25hCollecting onnxruntime<=1.8.0,==1.8.0\n","  Downloading onnxruntime-1.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n","\u001b[K     |████████████████████████████████| 4.5 MB 83.1 MB/s \n","\u001b[?25hRequirement already satisfied: sentencepiece<=0.1.96,>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from kobert==0.2.3) (0.1.91)\n","Requirement already satisfied: torch<=1.10.1,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from kobert==0.2.3) (1.8.1)\n","Collecting transformers<=4.8.1,>=4.8.1\n","  Downloading transformers-4.8.1-py3-none-any.whl (2.5 MB)\n","\u001b[K     |████████████████████████████████| 2.5 MB 51.7 MB/s \n","\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (3.19.6)\n","Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (1.21.6)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (1.12)\n","Collecting botocore<1.19.0,>=1.18.18\n","  Downloading botocore-1.18.18-py2.py3-none-any.whl (6.7 MB)\n","\u001b[K     |████████████████████████████████| 6.7 MB 75.1 MB/s \n","\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n","  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n","Collecting s3transfer<0.4.0,>=0.3.0\n","  Downloading s3transfer-0.3.7-py2.py3-none-any.whl (73 kB)\n","\u001b[K     |████████████████████████████████| 73 kB 2.2 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.19.0,>=1.18.18->boto3<=1.15.18->kobert==0.2.3) (2.8.2)\n","Requirement already satisfied: urllib3<1.26,>=1.20 in /usr/local/lib/python3.7/dist-packages (from botocore<1.19.0,>=1.18.18->boto3<=1.15.18->kobert==0.2.3) (1.24.3)\n","Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (0.29.32)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (21.3)\n","Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (0.8.4)\n","Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2.23.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.19.0,>=1.18.18->boto3<=1.15.18->kobert==0.2.3) (1.15.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2022.9.24)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<=1.10.1,>=1.7.0->kobert==0.2.3) (4.1.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (4.64.1)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (0.0.53)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (2022.6.2)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (4.13.0)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (0.10.3)\n","Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (0.0.12)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (3.8.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (6.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (3.10.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (1.2.0)\n","Building wheels for collected packages: kobert\n","  Building wheel for kobert (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for kobert: filename=kobert-0.2.3-py3-none-any.whl size=15708 sha256=cfb952c4dea26aefcfb013e2ade4730233b5a0752478d80599954df69eb5ff1c\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-y6zbc7f9/wheels/d3/68/ca/334747dfb038313b49cf71f84832a33372f3470d9ddfd051c0\n","Successfully built kobert\n","Installing collected packages: jmespath, botocore, s3transfer, transformers, onnxruntime, mxnet, boto3, kobert\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.8.2\n","    Uninstalling transformers-4.8.2:\n","      Successfully uninstalled transformers-4.8.2\n","  Attempting uninstall: mxnet\n","    Found existing installation: mxnet 1.9.1\n","    Uninstalling mxnet-1.9.1:\n","      Successfully uninstalled mxnet-1.9.1\n","Successfully installed boto3-1.15.18 botocore-1.18.18 jmespath-0.10.0 kobert-0.2.3 mxnet-1.7.0.post2 onnxruntime-1.8.0 s3transfer-0.3.7 transformers-4.8.1\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"NHm792AWAIsC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd '/gdrive/My Drive/KoBERT/checkpoint/'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mzg0ZzrE_FVm","executionInfo":{"status":"ok","timestamp":1668100060354,"user_tz":-540,"elapsed":283,"user":{"displayName":"MPS special","userId":"14769746348235507214"}},"outputId":"c70b1a11-bc47-497b-c47c-75fd454dbe06"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/gdrive/My Drive/KoBERT/checkpoint\n"]}]},{"cell_type":"code","source":["import gluonnlp as nlp\n","from kobert.utils import get_tokenizer\n","from kobert import get_onnx_kobert_model\n","\n","onnx_path = get_onnx_kobert_model()\n","sess = onnxruntime.InferenceSession(onnx_path)\n","\n","tokenizer = get_tokenizer()\n","tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":266},"id":"tEix92usnntf","executionInfo":{"status":"error","timestamp":1668157898489,"user_tz":-540,"elapsed":9142,"user":{"displayName":"MPS special","userId":"14769746348235507214"}},"outputId":"058e0215-c405-4898-f9a5-5f7c14399aba"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/.cache/kobert.onnx1.8.0.onnx[██████████████████████████████████████████████████]\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-0f09f13204e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkobert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_onnx_kobert_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mbertmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_onnx_kobert_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zRCzqETNCJUd"},"outputs":[],"source":["# tokenizing\n","from torch.utils.data import Dataset, DataLoader\n","\n","class EmotionDataset(Dataset):\n","  def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, vocab, max_len, pad, pair):\n","    transform = nlp.data.BERTSentenceTransform(bert_tokenizer, \n","                                               max_seq_length=max_len, vocab=vocab, pad=pad, pair=pair) \n","\n","    self.sentences = [transform([i[sent_idx]]) for i in dataset]\n","    self.labels = [np.int32(i[label_idx]) for i in dataset]\n","\n","  def __getitem__(self, i):\n","    return (self.sentences[i] + (self.labels[i], ))\n","\n","  def __len__(self):\n","    return (len(self.labels))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XNY9TmyWpnd9"},"outputs":[],"source":["# classifier\n","import torch\n","from torch import nn\n","\n","class EmotionClassifier(nn.Module):\n","  def __init__(self,\n","               bert,\n","               hidden_size = 768,\n","               num_classes = 9,\n","               dr_rate = None,\n","               params = None):\n","    super(EmotionClassifier, self).__init__()\n","    self.bert = bert\n","    self.dr_rate = dr_rate\n","\n","    self.classifier = nn.Linear(hidden_size, num_classes)\n","    if dr_rate:\n","      self.dropout = nn.Dropout(p=dr_rate)\n","\n","  def gen_attention_mask(self, token_ids, valid_length):\n","    attention_mask = torch.zeros_like(token_ids)\n","    for i, v in enumerate(valid_length):\n","      attention_mask[i][:v] = 1\n","    return attention_mask.float()\n","\n","  def forward(self, token_ids, valid_length, segment_ids):\n","    attention_mask = self.gen_attention_mask(token_ids, valid_length)\n","    \n","    _, pooler = self.bert(input_ids=token_ids, token_type_ids=segment_ids.long(), \n","                          attention_mask=attention_mask.float().to(token_ids.device), return_dict=False)\n","    \n","    if self.dr_rate:\n","      out = self.dropout(pooler)\n","    \n","    return self.classifier(out)"]},{"cell_type":"code","source":["# test\n","import torch\n","\n","# load model\n","model1 = torch.load('kobert_v2.pt', map_location=torch.device('cpu'))\n","model1.load_state_dict(torch.load('kobert_v2_state_dict.pt', map_location=torch.device('cpu')))"],"metadata":{"id":"P4HAal-oW7XQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668100092190,"user_tz":-540,"elapsed":6745,"user":{"displayName":"MPS special","userId":"14769746348235507214"}},"outputId":"6e8bb0ef-5e38-4573-a8b6-af1656592151"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["import numpy as np\n","\n","def torch_to_onnx():\n","\n","    model1.eval()\n","\n","    max_len = 64\n","    batch_size = 32\n","\n","    data =['안녕하세요 테스트 문장입니다', '0']\n","    dataset_new = [data]\n","\n","    new_test = EmotionDataset(dataset_new, 0, 1, tok, vocab, max_len, True, False)\n","    test_dataloader = torch.utils.data.DataLoader(new_test, batch_size=batch_size, num_workers=4)\n","\n","    # dummy input x = 모델의 input = 모델의 forward 함수를 통과하는 타입\n","    for token_ids, valid_length, segment_ids, label in test_dataloader:   \n","        x = token_ids, valid_length, segment_ids\n","\n","    torch.onnx.export(\n","        model1,\n","        x,\n","        'model.onnx',\n","        export_params=True,\n","        # verbose=True,\n","        input_names=['token_ids', 'valid_length', 'segment_ids'],\n","        output_names=['output'],\n","        opset_version=13\n","    )"],"metadata":{"id":"w4Ezz4PshiNj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_len = 64\n","batch_size = 32\n","\n","data =['안녕하세요 테스트 문장입니다', '0']\n","dataset_new = [data]\n","\n","new_test = EmotionDataset(dataset_new, 0, 1, tok, vocab, max_len, True, False)\n","test_dataloader = torch.utils.data.DataLoader(new_test, batch_size=batch_size, num_workers=4)\n","\n","for token_ids, valid_length, segment_ids, label in test_dataloader:   \n","    x = token_ids, valid_length, segment_ids\n","    y = model1(token_ids, valid_length, segment_ids)\n","\n","print(x)\n","print(y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pgGLUnVKQiGt","executionInfo":{"status":"ok","timestamp":1668100097853,"user_tz":-540,"elapsed":1595,"user":{"displayName":"MPS special","userId":"14769746348235507214"}},"outputId":"dcfc8c85-c63e-4c5c-8df0-30367fbf1449"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(tensor([[   2, 3135, 5724, 7814, 4736, 2120, 7178, 7139,    3,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1,    1,    1]], dtype=torch.int32), tensor([9], dtype=torch.int32), tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.int32))\n","tensor([[ 3.8171, -2.0730, -0.8213, -0.1868, -0.6384, -0.4231,  3.3142, -0.6208,\n","         -0.9593]], grad_fn=<AddmmBackward>)\n"]}]},{"cell_type":"code","source":["torch_to_onnx()"],"metadata":{"id":"0T-uDz3zC9qz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predict('안녕하세요 테스트 문장입니다')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"id":"6DGRDceL_0f2","executionInfo":{"status":"ok","timestamp":1668091756508,"user_tz":-540,"elapsed":890,"user":{"displayName":"MPS special","userId":"14769746348235507214"}},"outputId":"93889386-bcd9-41d6-925c-7e53de380fdc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["kobert_v2 : [3.817, 0, 0, 0, 0, 0, 3.314, 0, 0] 중립\n"]},{"output_type":"execute_result","data":{"text/plain":["'중립'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["import onnx\n","\n","from onnx import helper as h\n","from onnx import checker as ch\n","from onnx import TensorProto, GraphProto\n","from onnx import numpy_helper as nph\n","\n","import numpy as np\n","from collections import OrderedDict\n","\n","# from logger import log\n","# import typer\n","\n","\n","def make_param_dictionary(initializer):\n","    params = OrderedDict()\n","    for data in initializer:\n","        params[data.name] = data\n","    return params\n","\n","\n","def convert_params_to_int32(params_dict):\n","    converted_params = []\n","    for param in params_dict:\n","        data = params_dict[param]\n","        if data.data_type == TensorProto.INT64:\n","            data_cvt = nph.to_array(data).astype(np.int32)\n","            data = nph.from_array(data_cvt, data.name)\n","        converted_params += [data]\n","    return converted_params\n","\n","\n","def convert_constant_nodes_to_int32(nodes):\n","    \"\"\"\n","    convert_constant_nodes_to_int32 Convert Constant nodes to INT32. If a constant node has data type INT64, a new version of the\n","    node is created with INT32 data type and stored.\n","\n","    Args:\n","        nodes (list): list of nodes\n","\n","    Returns:\n","        list: list of new nodes all with INT32 constants.\n","    \"\"\"\n","    new_nodes = []\n","    for node in nodes:\n","        if (\n","            node.op_type == \"Constant\"\n","            and node.attribute[0].t.data_type == TensorProto.INT64\n","        ):\n","            data = nph.to_array(node.attribute[0].t).astype(np.int32)\n","            new_t = nph.from_array(data)\n","            new_node = h.make_node(\n","                \"Constant\",\n","                inputs=[],\n","                outputs=node.output,\n","                name=node.name,\n","                value=new_t,\n","            )\n","            new_nodes += [new_node]\n","        else:\n","            new_nodes += [node]\n","\n","    return new_nodes\n","\n","\n","def convert_model_to_int32(model_path: str, out_path: str):\n","    \"\"\"\n","    convert_model_to_int32 Converts ONNX model with INT64 params to INT32 params.\\n\n","\n","    Args:\\n\n","        model_path (str): path to original ONNX model.\\n\n","        out_path (str): path to save converted model.\n","    \"\"\"\n","    # log.info(\"ONNX INT64 --> INT32 Converter\")\n","    # log.info(f\"Loading Model: {model_path}\")\n","    # * load model.\n","    model = onnx.load_model(model_path)\n","    ch.check_model(model)\n","    # * get model opset version.\n","    opset_version = model.opset_import[0].version\n","    graph = model.graph\n","    # * The initializer holds all non-constant weights.\n","    init = graph.initializer\n","    # * collect model params in a dictionary.\n","    params_dict = make_param_dictionary(init)\n","    # log.info(\"Converting INT64 model params to INT32...\")\n","    # * convert all INT64 aprams to INT32.\n","    converted_params = convert_params_to_int32(params_dict)\n","    # log.info(\"Converting constant INT64 nodes to INT32...\")\n","    new_nodes = convert_constant_nodes_to_int32(graph.node)\n","\n","    graph_name = f\"{graph.name}-int32\"\n","    # log.info(\"Creating new graph...\")\n","    # * create a new graph with converted params and new nodes.\n","    graph_int32 = h.make_graph(\n","        new_nodes,\n","        graph_name,\n","        graph.input,\n","        graph.output,\n","        initializer=converted_params,\n","    )\n","    # log.info(\"Creating new int32 model...\")\n","    model_int32 = h.make_model(graph_int32, producer_name=\"onnx-typecast\")\n","    model_int32.opset_import[0].version = opset_version\n","    ch.check_model(model_int32)\n","    # log.info(f\"Saving converted model as: {out_path}\")\n","    onnx.save_model(model_int32, out_path)\n","    # log.info(f\"Done Done London. 🎉\")\n","    return"],"metadata":{"id":"fu_wrhsRY3KR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["convert_model_to_int32('model.onnx', 'model_32.onnx')"],"metadata":{"id":"mFjl-LR9Z0fw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import onnx\n","\n","onnx_model = onnx.load('model_32.onnx')\n","onnx.checker.check_model(onnx_model)"],"metadata":{"id":"aV56jtkvNmHj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install onnxruntime==1.7.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Imdzvj5vgZ66","executionInfo":{"status":"ok","timestamp":1668100163403,"user_tz":-540,"elapsed":3899,"user":{"displayName":"MPS special","userId":"14769746348235507214"}},"outputId":"09b1100e-4f2c-448f-d954-4db4020ce72a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: onnxruntime==1.7.0 in /usr/local/lib/python3.7/dist-packages (1.7.0)\n","Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from onnxruntime==1.7.0) (1.18.5)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from onnxruntime==1.7.0) (3.17.3)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->onnxruntime==1.7.0) (1.15.0)\n"]}]},{"cell_type":"code","source":["import onnxruntime\n","\n","onnxruntime.__version__"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"cfxVs4RSgFYV","executionInfo":{"status":"ok","timestamp":1668100195228,"user_tz":-540,"elapsed":634,"user":{"displayName":"MPS special","userId":"14769746348235507214"}},"outputId":"b6e12d10-4005-44b9-e2ef-2f6446e4d234"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'1.7.0'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["ort_session = onnxruntime.InferenceSession('model_32.onnx')\n","\n","def to_numpy(tensor):\n","    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n","\n","ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)}\n","ort_outs = ort_session.run(None, ort_inputs)\n","\n","np.testing.assert_allclose(to_numpy(y), ort_outs[0], rtol=1e-03, atol=1e-05)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":508},"id":"J6mNxELoP2Yw","executionInfo":{"status":"error","timestamp":1668100956875,"user_tz":-540,"elapsed":9,"user":{"displayName":"MPS special","userId":"14769746348235507214"}},"outputId":"661b942d-7d35-469e-fdba-5f2066d1e458"},"execution_count":null,"outputs":[{"output_type":"error","ename":"InvalidGraph","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidGraph\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-2417a4dd4f9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mort_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monnxruntime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInferenceSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_32.onnx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_bytes, sess_options, providers, provider_options)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_inference_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproviders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprovider_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_fallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py\u001b[0m in \u001b[0;36m_create_inference_session\u001b[0;34m(self, providers, provider_options)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0msession_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess_options\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess_options\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_session_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInferenceSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_config_from_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInferenceSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_config_from_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidGraph\u001b[0m: [ONNXRuntimeError] : 10 : INVALID_GRAPH : Load model from model_32.onnx failed:This is an invalid model. Type Error: Type 'tensor(int32)' of input parameter (207) of operator (Split) in node (Split_3) is invalid."]}]},{"cell_type":"code","source":[],"metadata":{"id":"rwypXdcDQosm"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"machine_shape":"hm","provenance":[{"file_id":"1WMQTRgIn7HBZ1Tp2vOLMOZtuBN2-pO1E","timestamp":1666697884707}],"authorship_tag":"ABX9TyNS3pEifSWp5TzF+zuy595Y"},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}