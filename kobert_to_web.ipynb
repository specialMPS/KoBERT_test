{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23965,"status":"ok","timestamp":1668476322122,"user":{"displayName":"MPS special","userId":"14769746348235507214"},"user_tz":-540},"id":"Z5rd8sR5mD9t","outputId":"21807123-1d81-4241-db79-f2ede7e19375"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n"]}],"source":["# google drive mount\n","\n","from google.colab import drive\n","drive.mount('/gdrive', force_remount=True)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16406,"status":"ok","timestamp":1668476338516,"user":{"displayName":"MPS special","userId":"14769746348235507214"},"user_tz":-540},"id":"-Ppy5Tgjkykp","outputId":"bef2105c-8fea-438e-f1bc-6846a4a0568b"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 9.2 MB 4.6 MB/s \n","\u001b[K     |████████████████████████████████| 164 kB 59.8 MB/s \n","\u001b[K     |████████████████████████████████| 4.7 MB 59.2 MB/s \n","\u001b[K     |████████████████████████████████| 78 kB 4.0 MB/s \n","\u001b[K     |████████████████████████████████| 182 kB 19.6 MB/s \n","\u001b[K     |████████████████████████████████| 237 kB 54.0 MB/s \n","\u001b[K     |████████████████████████████████| 63 kB 1.5 MB/s \n","\u001b[K     |████████████████████████████████| 51 kB 6.4 MB/s \n","\u001b[?25h  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyngrok\n","  Downloading pyngrok-5.1.0.tar.gz (745 kB)\n","\u001b[K     |████████████████████████████████| 745 kB 7.9 MB/s \n","\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyngrok) (6.0)\n","Building wheels for collected packages: pyngrok\n","  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyngrok: filename=pyngrok-5.1.0-py3-none-any.whl size=19007 sha256=32192782063f3f17f476aab8ad21dd75c5ab1528b0d958869c735e8d7b47e1f6\n","  Stored in directory: /root/.cache/pip/wheels/bf/e6/af/ccf6598ecefecd44104069371795cb9b3afbcd16987f6ccfb3\n","Successfully built pyngrok\n","Installing collected packages: pyngrok\n","Successfully installed pyngrok-5.1.0\n"]}],"source":["!pip install streamlit -q\n","!pip install pyngrok"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"UAz8e4WAlPIK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668476344206,"user_tz":-540,"elapsed":5703,"user":{"displayName":"MPS special","userId":"14769746348235507214"}},"outputId":"1cc7d9d0-90b3-40c6-d748-f3af2837f7e0"},"outputs":[{"output_type":"stream","name":"stdout","text":[]}],"source":["from pyngrok import ngrok\n","\n","ngrok.set_auth_token('2BVeByVJG3w1QaQFmdREM9308C0_4V7edUh4hAybvpQ5dqRCn')"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":142289,"status":"ok","timestamp":1668476486481,"user":{"displayName":"MPS special","userId":"14769746348235507214"},"user_tz":-540},"id":"Ga53XtyGmJl9","outputId":"7c015c0d-0375-4e6b-d4bb-ee2f30df5c3d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting gluonnlp==0.10.0\n","  Downloading gluonnlp-0.10.0.tar.gz (344 kB)\n","\u001b[K     |████████████████████████████████| 344 kB 8.1 MB/s \n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.1)\n","Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from gluonnlp==0.10.0) (1.21.6)\n","Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from gluonnlp==0.10.0) (0.29.32)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gluonnlp==0.10.0) (21.3)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.6)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->gluonnlp==0.10.0) (3.0.9)\n","Building wheels for collected packages: gluonnlp\n","  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp37-cp37m-linux_x86_64.whl size=595741 sha256=72adb8bbb39562d173d6f6d47fec46ce1eb3f0186816a9e394abb16cc3e75162\n","  Stored in directory: /root/.cache/pip/wheels/be/b4/06/7f3fdfaf707e6b5e98b79c041e023acffbe395d78a527eae00\n","Successfully built gluonnlp\n","Installing collected packages: gluonnlp\n","Successfully installed gluonnlp-0.10.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting mxnet\n","  Downloading mxnet-1.9.1-py3-none-manylinux2014_x86_64.whl (49.1 MB)\n","\u001b[K     |████████████████████████████████| 49.1 MB 1.5 MB/s \n","\u001b[?25hCollecting graphviz<0.9.0,>=0.8.1\n","  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.7/dist-packages (from mxnet) (1.21.6)\n","Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet) (2.23.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n","Installing collected packages: graphviz, mxnet\n","  Attempting uninstall: graphviz\n","    Found existing installation: graphviz 0.10.1\n","    Uninstalling graphviz-0.10.1:\n","      Successfully uninstalled graphviz-0.10.1\n","Successfully installed graphviz-0.8.4 mxnet-1.9.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece==0.1.91\n","  Downloading sentencepiece-0.1.91-cp37-cp37m-manylinux1_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 7.3 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.91\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torch==1.8.1\n","  Downloading torch-1.8.1-cp37-cp37m-manylinux1_x86_64.whl (804.1 MB)\n","\u001b[K     |████████████████████████████████| 804.1 MB 2.3 kB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1) (1.21.6)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1) (4.1.1)\n","Installing collected packages: torch\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.12.1+cu113\n","    Uninstalling torch-1.12.1+cu113:\n","      Successfully uninstalled torch-1.12.1+cu113\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 1.8.1 which is incompatible.\n","torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.8.1 which is incompatible.\n","torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.8.1 which is incompatible.\u001b[0m\n","Successfully installed torch-1.8.1\n"]}],"source":["!pip install gluonnlp==0.10.0 pandas tqdm   \n","!pip install mxnet\n","!pip install sentencepiece==0.1.91\n","# !pip install transformers==4.8.2\n","!pip install torch==1.8.1"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30608,"status":"ok","timestamp":1668476517068,"user":{"displayName":"MPS special","userId":"14769746348235507214"},"user_tz":-540},"id":"RpftqirpmQOD","outputId":"26e9d327-988c-4eea-eda3-64560a3665e5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n","  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-572rd8kf\n","  Running command git clone -q 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-572rd8kf\n","Collecting boto3<=1.15.18\n","  Downloading boto3-1.15.18-py2.py3-none-any.whl (129 kB)\n","\u001b[K     |████████████████████████████████| 129 kB 7.7 MB/s \n","\u001b[?25hRequirement already satisfied: gluonnlp<=0.10.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from kobert==0.2.3) (0.10.0)\n","Collecting mxnet<=1.7.0.post2,>=1.4.0\n","  Downloading mxnet-1.7.0.post2-py2.py3-none-manylinux2014_x86_64.whl (54.7 MB)\n","\u001b[K     |████████████████████████████████| 54.7 MB 17 kB/s \n","\u001b[?25hCollecting onnxruntime<=1.8.0,==1.8.0\n","  Downloading onnxruntime-1.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n","\u001b[K     |████████████████████████████████| 4.5 MB 36.6 MB/s \n","\u001b[?25hRequirement already satisfied: sentencepiece<=0.1.96,>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from kobert==0.2.3) (0.1.91)\n","Requirement already satisfied: torch<=1.10.1,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from kobert==0.2.3) (1.8.1)\n","Collecting transformers<=4.8.1,>=4.8.1\n","  Downloading transformers-4.8.1-py3-none-any.whl (2.5 MB)\n","\u001b[K     |████████████████████████████████| 2.5 MB 38.7 MB/s \n","\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (3.19.6)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (1.12)\n","Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (1.21.6)\n","Collecting s3transfer<0.4.0,>=0.3.0\n","  Downloading s3transfer-0.3.7-py2.py3-none-any.whl (73 kB)\n","\u001b[K     |████████████████████████████████| 73 kB 1.7 MB/s \n","\u001b[?25hCollecting botocore<1.19.0,>=1.18.18\n","  Downloading botocore-1.18.18-py2.py3-none-any.whl (6.7 MB)\n","\u001b[K     |████████████████████████████████| 6.7 MB 31.6 MB/s \n","\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n","  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.19.0,>=1.18.18->boto3<=1.15.18->kobert==0.2.3) (2.8.2)\n","Requirement already satisfied: urllib3<1.26,>=1.20 in /usr/local/lib/python3.7/dist-packages (from botocore<1.19.0,>=1.18.18->boto3<=1.15.18->kobert==0.2.3) (1.24.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (21.3)\n","Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (0.29.32)\n","Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (0.8.4)\n","Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2.23.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.19.0,>=1.18.18->boto3<=1.15.18->kobert==0.2.3) (1.15.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2022.9.24)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<=1.10.1,>=1.7.0->kobert==0.2.3) (4.1.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (4.64.1)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 46.4 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (3.8.0)\n","Collecting huggingface-hub==0.0.12\n","  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (2022.6.2)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (4.13.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[K     |████████████████████████████████| 880 kB 48.0 MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (6.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (3.10.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (1.2.0)\n","Building wheels for collected packages: kobert, sacremoses\n","  Building wheel for kobert (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for kobert: filename=kobert-0.2.3-py3-none-any.whl size=15708 sha256=e6cffbb46a931fe10a684d65ea378f63ad64f618bb525a760de2d80186bdd6b0\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-xavb0427/wheels/d3/68/ca/334747dfb038313b49cf71f84832a33372f3470d9ddfd051c0\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895259 sha256=c375c74bf4563271aefa08531c214756c3440bbb217357f708c90f3068907eda\n","  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n","Successfully built kobert sacremoses\n","Installing collected packages: jmespath, botocore, tokenizers, sacremoses, s3transfer, huggingface-hub, transformers, onnxruntime, mxnet, boto3, kobert\n","  Attempting uninstall: mxnet\n","    Found existing installation: mxnet 1.9.1\n","    Uninstalling mxnet-1.9.1:\n","      Successfully uninstalled mxnet-1.9.1\n","Successfully installed boto3-1.15.18 botocore-1.18.18 huggingface-hub-0.0.12 jmespath-0.10.0 kobert-0.2.3 mxnet-1.7.0.post2 onnxruntime-1.8.0 s3transfer-0.3.7 sacremoses-0.0.53 tokenizers-0.10.3 transformers-4.8.1\n"]}],"source":["!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aUQGi-nXmZXE","outputId":"6ddba048-21a0-48b4-d58e-ef3cf99ed581","executionInfo":{"status":"ok","timestamp":1668476522485,"user_tz":-540,"elapsed":5449,"user":{"displayName":"MPS special","userId":"14769746348235507214"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/ssut/py-hanspell.git\n","  Cloning https://github.com/ssut/py-hanspell.git to /tmp/pip-req-build-1lpiw94d\n","  Running command git clone -q https://github.com/ssut/py-hanspell.git /tmp/pip-req-build-1lpiw94d\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from py-hanspell==1.1) (2.23.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->py-hanspell==1.1) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->py-hanspell==1.1) (2022.9.24)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->py-hanspell==1.1) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->py-hanspell==1.1) (3.0.4)\n","Building wheels for collected packages: py-hanspell\n","  Building wheel for py-hanspell (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for py-hanspell: filename=py_hanspell-1.1-py3-none-any.whl size=4869 sha256=b803ce4f222344f2e8cd027eb20a3ad5cc7e69e8c50c5f716e5746a6b8799dba\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-_7z62p6c/wheels/ab/f5/7b/d4124bb329c905301baed80e2ae45aa14e824f62ebc3ec2cc4\n","Successfully built py-hanspell\n","Installing collected packages: py-hanspell\n","Successfully installed py-hanspell-1.1\n"]}],"source":["!pip install git+https://github.com/ssut/py-hanspell.git"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"D23Y64yNlbvd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668476524435,"user_tz":-540,"elapsed":15,"user":{"displayName":"MPS special","userId":"14769746348235507214"}},"outputId":"287f2fda-5eb1-444c-8f52-5b5805b65685"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing app.py\n"]}],"source":["%%writefile app.py\n","\n","import streamlit as st\n","from hanspell import spell_checker\n","import re\n","import torch\n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader\n","import gluonnlp as nlp\n","import numpy as np\n","import pandas as pd\n","\n","from kobert.utils import get_tokenizer\n","from kobert.pytorch_kobert import get_pytorch_kobert_model\n","\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","\n","# tokenizing\n","class EmotionDataset(Dataset):\n","  def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len, pad, pair):\n","    transform = nlp.data.BERTSentenceTransform(bert_tokenizer, \n","                                               max_seq_length=max_len, pad=pad, pair=pair) \n","\n","    self.sentences = [transform([i[sent_idx]]) for i in dataset]\n","    self.labels = [np.int32(i[label_idx]) for i in dataset]\n","\n","  def __getitem__(self, i):\n","    return (self.sentences[i] + (self.labels[i], ))\n","\n","  def __len__(self):\n","    return (len(self.labels))\n","\n","\n","# classifier\n","class EmotionClassifier(nn.Module):\n","  def __init__(self,\n","               bert,\n","               hidden_size = 768,\n","               num_classes = 9,\n","               dr_rate = None,\n","               params = None):\n","    super(EmotionClassifier, self).__init__()\n","    self.bert = bert\n","    self.dr_rate = dr_rate\n","\n","    self.classifier = nn.Linear(hidden_size, num_classes)\n","    if dr_rate:\n","      self.dropout = nn.Dropout(p=dr_rate)\n","\n","  def gen_attention_mask(self, token_ids, valid_length):\n","    attention_mask = torch.zeros_like(token_ids)\n","    for i, v in enumerate(valid_length):\n","      attention_mask[i][:v] = 1\n","    return attention_mask.float()\n","\n","  def forward(self, token_ids, valid_length, segment_ids):\n","    attention_mask = self.gen_attention_mask(token_ids, valid_length)\n","    \n","    _, pooler = self.bert(input_ids=token_ids, token_type_ids=segment_ids.long(), \n","                          attention_mask=attention_mask.float().to(token_ids.device), return_dict=False)\n","    \n","    if self.dr_rate:\n","      out = self.dropout(pooler)\n","    \n","    return self.classifier(out)\n","\n","\n","@st.cache(allow_output_mutation=True)\n","def loading_model():\n","    bertmodel, vocab = get_pytorch_kobert_model()\n","\n","    tokenizer = get_tokenizer()\n","    tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n","\n","    return tok\n","\n","\n","def cleaning(predict_sentence):\n","\n","    # 숫자, 문자 아닌 글자 제거\n","    removed_sent = re.sub('[^\\w\\s]', '', predict_sentence).strip()\n","\n","    # 맞춤법 교정\n","    spelled_sent = spell_checker.check(removed_sent)\n","    checked_sent = spelled_sent.checked\n","\n","    return checked_sent\n","\n","\n","def predict(predict_sentence, model):\n","\n","    max_len = 64\n","    batch_size = 32\n","\n","    # flipped emotion dict\n","    emotion_labels = {'중립': 0,\n","                      '기쁨': 1,\n","                      '놀람': 2,\n","                      '긴장됨': 3,\n","                      '괴로움': 4,\n","                      '화남': 5,\n","                      '비참함': 6,\n","                      '우울함': 7,\n","                      '피로함': 8}\n","    flip_emotion_labels = {v: k for k, v in emotion_labels.items()}\n","\n","    data =[predict_sentence, '0']\n","    dataset_new = [data]\n","\n","    tok = loading_model()\n","\n","    new_test = EmotionDataset(dataset_new, 0, 1, tok, max_len, True, False)\n","    test_dataloader = torch.utils.data.DataLoader(new_test, batch_size=batch_size, num_workers=5)\n","\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","\n","        valid_length = valid_length\n","        label = label.long().to(device)\n","\n","        out1 = model(token_ids, valid_length, segment_ids)\n","        for i in out1:\n","            logits = i\n","            logits = logits.detach().cpu().numpy()\n","\n","            logits_zero = [np.round(x, 3) if x > 0 else 0 for x in logits]\n","            # print(logits_zero, flip_emotion_labels[np.argmax(logits)])\n","\n","            return flip_emotion_labels[np.argmax(logits)]\n","\n","\n","@st.cache(allow_output_mutation=True)\n","def load_kobert_model():\n","    # load model\n","    model = torch.load('/gdrive/My Drive/KoBERT/checkpoint/kobert_v2.pt', map_location=device)\n","    model.load_state_dict(torch.load('/gdrive/My Drive/KoBERT/checkpoint/kobert_v2_state_dict.pt', map_location=device))\n","    model.eval()\n","\n","    return model\n","\n","\n","if __name__ == '__main__':\n","\n","  kobert_model = load_kobert_model()\n","\n","  st.markdown(\"<h1 style='text-align: center;'>BERT 기반 감정 분석 모델</h1>\", unsafe_allow_html=True)\n","  st.write('\\n\\n\\n')\n","  input_sent = st.text_input('분석하고 싶은 문장을 입력해주세요!', '')\n","\n","  if input_sent is None:\n","      st.text('문장을 입력해주세요')\n","  else:\n","      st.write('\\n\\n\\n')\n","      st.subheader('감정 분석 결과')\n","      emotion = st.text('...')\n","      pred = predict(cleaning(input_sent), kobert_model)\n","      emotion.text(pred)\n"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"3b5SNM-FqlXa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668478920406,"user_tz":-540,"elapsed":338,"user":{"displayName":"MPS special","userId":"14769746348235507214"}},"outputId":"453d3001-d7fa-49ef-9b7a-d2a4ff59e9a5"},"outputs":[{"output_type":"stream","name":"stdout","text":["app.py\tnohup.out  sample_data\n"]}],"source":["!ls"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"aTJNf7XKql1Y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668478924136,"user_tz":-540,"elapsed":1190,"user":{"displayName":"MPS special","userId":"14769746348235507214"}},"outputId":"4c3d38bc-0968-4e1a-87e7-b1e2c8c4e868"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","import streamlit as st\n","from hanspell import spell_checker\n","import re\n","import torch\n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader\n","import gluonnlp as nlp\n","import numpy as np\n","import pandas as pd\n","\n","from kobert.utils import get_tokenizer\n","from kobert.pytorch_kobert import get_pytorch_kobert_model\n","\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","\n","# tokenizing\n","class EmotionDataset(Dataset):\n","  def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len, pad, pair):\n","    transform = nlp.data.BERTSentenceTransform(bert_tokenizer, \n","                                               max_seq_length=max_len, pad=pad, pair=pair) \n","\n","    self.sentences = [transform([i[sent_idx]]) for i in dataset]\n","    self.labels = [np.int32(i[label_idx]) for i in dataset]\n","\n","  def __getitem__(self, i):\n","    return (self.sentences[i] + (self.labels[i], ))\n","\n","  def __len__(self):\n","    return (len(self.labels))\n","\n","\n","# classifier\n","class EmotionClassifier(nn.Module):\n","  def __init__(self,\n","               bert,\n","               hidden_size = 768,\n","               num_classes = 9,\n","               dr_rate = None,\n","               params = None):\n","    super(EmotionClassifier, self).__init__()\n","    self.bert = bert\n","    self.dr_rate = dr_rate\n","\n","    self.classifier = nn.Linear(hidden_size, num_classes)\n","    if dr_rate:\n","      self.dropout = nn.Dropout(p=dr_rate)\n","\n","  def gen_attention_mask(self, token_ids, valid_length):\n","    attention_mask = torch.zeros_like(token_ids)\n","    for i, v in enumerate(valid_length):\n","      attention_mask[i][:v] = 1\n","    return attention_mask.float()\n","\n","  def forward(self, token_ids, valid_length, segment_ids):\n","    attention_mask = self.gen_attention_mask(token_ids, valid_length)\n","    \n","    _, pooler = self.bert(input_ids=token_ids, token_type_ids=segment_ids.long(), \n","                          attention_mask=attention_mask.float().to(token_ids.device), return_dict=False)\n","    \n","    if self.dr_rate:\n","      out = self.dropout(pooler)\n","    \n","    return self.classifier(out)\n","\n","\n","@st.cache(allow_output_mutation=True)\n","def loading_model():\n","    bertmodel, vocab = get_pytorch_kobert_model()\n","\n","    tokenizer = get_tokenizer()\n","    tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n","\n","    return tok\n","\n","\n","def cleaning(predict_sentence):\n","\n","    # 숫자, 문자 아닌 글자 제거\n","    removed_sent = re.sub('[^\\w\\s]', '', predict_sentence).strip()\n","\n","    # 맞춤법 교정\n","    spelled_sent = spell_checker.check(removed_sent)\n","    checked_sent = spelled_sent.checked\n","\n","    return checked_sent\n","\n","\n","def predict(predict_sentence, model):\n","\n","    max_len = 64\n","    batch_size = 32\n","\n","    # flipped emotion dict\n","    emotion_labels = {'중립': 0,\n","                      '기쁨': 1,\n","                      '놀람': 2,\n","                      '긴장됨': 3,\n","                      '괴로움': 4,\n","                      '화남': 5,\n","                      '비참함': 6,\n","                      '우울함': 7,\n","                      '피로함': 8}\n","    flip_emotion_labels = {v: k for k, v in emotion_labels.items()}\n","\n","    data =[predict_sentence, '0']\n","    dataset_new = [data]\n","\n","    tok = loading_model()\n","\n","    new_test = EmotionDataset(dataset_new, 0, 1, tok, max_len, True, False)\n","    test_dataloader = torch.utils.data.DataLoader(new_test, batch_size=batch_size, num_workers=5)\n","\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","\n","        valid_length = valid_length\n","        label = label.long().to(device)\n","\n","        out1 = model(token_ids, valid_length, segment_ids)\n","        for i in out1:\n","            logits = i\n","            logits = logits.detach().cpu().numpy()\n","\n","            logits_zero = [np.round(x, 3) if x > 0 else 0 for x in logits]\n","            # print(logits_zero, flip_emotion_labels[np.argmax(logits)])\n","\n","            return flip_emotion_labels[np.argmax(logits)]\n","\n","\n","@st.cache(allow_output_mutation=True)\n","def load_kobert_model():\n","    # load model\n","    model = torch.load('/gdrive/My Drive/KoBERT/checkpoint/kobert_v2.pt', map_location=device)\n","    model.load_state_dict(torch.load('/gdrive/My Drive/KoBERT/checkpoint/kobert_v2_state_dict.pt', map_location=device))\n","    model.eval()\n","\n","    return model\n","\n","\n","if __name__ == '__main__':\n","\n","  kobert_model = load_kobert_model()\n","\n","  st.markdown(\"<h1 style='text-align: center;'>BERT 기반 감정 분석 모델</h1>\", unsafe_allow_html=True)\n","  st.write('\\n\\n\\n')\n","  input_sent = st.text_input('분석하고 싶은 문장을 입력해주세요!', '')\n","\n","  if input_sent is None:\n","      st.text('문장을 입력해주세요')\n","  else:\n","      st.write('\\n\\n\\n')\n","      st.subheader('감정 분석 결과')\n","      emotion = st.text('...')\n","      pred = predict(cleaning(input_sent), kobert_model)\n","      emotion.text(pred)\n"]}],"source":["!cat app.py"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"tBaOa35oqnPY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668478928615,"user_tz":-540,"elapsed":358,"user":{"displayName":"MPS special","userId":"14769746348235507214"}},"outputId":"d6c19830-d388-4b24-d96a-0de879cebb98"},"outputs":[{"output_type":"stream","name":"stdout","text":["nohup: appending output to 'nohup.out'\n"]}],"source":["!nohup streamlit run app.py --server.port 80 &"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"FHjaP-cFqudj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668478931427,"user_tz":-540,"elapsed":532,"user":{"displayName":"MPS special","userId":"14769746348235507214"}},"outputId":"2c5c5e50-2be6-422d-8d9f-8a50e63190a5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<NgrokTunnel: \"http://024a-34-125-236-105.ngrok.io\" -> \"http://localhost:80\">"]},"metadata":{},"execution_count":18}],"source":["url = ngrok.connect(port='80')\n","url"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"9jzSG5ttqzVv","executionInfo":{"status":"ok","timestamp":1668478913001,"user_tz":-540,"elapsed":471,"user":{"displayName":"MPS special","userId":"14769746348235507214"}}},"outputs":[],"source":["ngrok.kill()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"idq1YPJlrSKP"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[],"authorship_tag":"ABX9TyMJPCPiPFA06nrWd2vqJtDd"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}