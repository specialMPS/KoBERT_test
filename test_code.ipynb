{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPZFG7fthGm5tXL92mwKT39"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install kobert-transformers==0.4.1\n","!pip install transformers==3.0.2\n","!pip install torch\n","!pip install tokenizers==0.8.1rc1"],"metadata":{"id":"BBbLxpNP_9EE"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jLbsVhxe_vn7"},"outputs":[],"source":["# configuration\n","\n","from transformers import BertConfig\n","\n","\n","kobert_config = {\n","    'attention_probs_dropout_prob': 0.1,\n","    'hidden_act': 'gelu',\n","    'hidden_dropout_prob': 0.1,\n","    'hidden_size': 768,\n","    'initializer_range': 0.02,\n","    'intermediate_size': 3072,\n","    'max_position_embeddings': 512,\n","    'num_attention_heads': 12,\n","    'num_hidden_layers': 12,\n","    'type_vocab_size': 2,\n","    'vocab_size': 8002\n","}\n","\n","def get_kobert_config():\n","    return BertConfig.from_dict(kobert_config)"]},{"cell_type":"code","source":["# dataloader\n","\n","import torch\n","from kobert_transformers import get_tokenizer\n","from torch.utils.data import Dataset\n","\n","\n","class WellnessTextClassificationDataset(Dataset):\n","    def __init__(self,\n","                 file_path=\"/gdrive/My Drive/KoBERT/data/wellness_dataset.txt\",\n","                 num_label=9,\n","                 device='cpu',\n","                 max_seq_len=512,  # KoBERT max_length\n","                 tokenizer=None\n","                 ):\n","        self.file_path = file_path\n","        self.device = device\n","        self.data = []\n","        self.tokenizer = tokenizer if tokenizer is not None else get_tokenizer()\n","\n","        file = open(self.file_path, 'r', encoding='cp949')\n","\n","        while True:\n","            line = file.readline()\n","            if not line:\n","                break\n","            datas = line.split(\"\\t\")\n","            index_of_words = self.tokenizer.encode(datas[0])\n","            token_type_ids = [0] * len(index_of_words)\n","            attention_mask = [1] * len(index_of_words)\n","\n","            # Padding Length\n","            padding_length = max_seq_len - len(index_of_words)\n","\n","            # Zero Padding\n","            index_of_words += [0] * padding_length\n","            token_type_ids += [0] * padding_length\n","            attention_mask += [0] * padding_length\n","\n","            # Label\n","            label = int(datas[1][:-1])\n","\n","            data = {\n","                'input_ids': torch.tensor(index_of_words).to(self.device),\n","                'token_type_ids': torch.tensor(token_type_ids).to(self.device),\n","                'attention_mask': torch.tensor(attention_mask).to(self.device),\n","                'labels': torch.tensor(label).to(self.device)\n","            }\n","\n","            self.data.append(data)\n","\n","        file.close()\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        item = self.data[index]\n","        return item\n","\n","\n","if __name__ == \"__main__\":\n","    dataset = WellnessTextClassificationDataset()\n","    print(dataset)"],"metadata":{"id":"q07uD3h7ADbt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# classifier\n","\n","import torch.nn as nn\n","from kobert_transformers import get_kobert_model\n","from torch.nn import CrossEntropyLoss, MSELoss\n","from transformers import BertPreTrainedModel\n","\n","\n","class KoBERTforSequenceClassfication(BertPreTrainedModel):\n","    def __init__(self,\n","                 num_labels=9,\n","                 hidden_size=768,\n","                 hidden_dropout_prob=0.1,\n","                 ):\n","        super().__init__(get_kobert_config())\n","\n","        self.num_labels = num_labels\n","        self.kobert = get_kobert_model()\n","        self.dropout = nn.Dropout(hidden_dropout_prob)\n","        self.classifier = nn.Linear(hidden_size, num_labels)\n","\n","        self.init_weights()\n","\n","    def forward(\n","            self,\n","            input_ids=None,\n","            attention_mask=None,\n","            token_type_ids=None,\n","            position_ids=None,\n","            head_mask=None,\n","            inputs_embeds=None,\n","            labels=None,\n","    ):\n","        outputs = self.kobert(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            head_mask=head_mask,\n","            inputs_embeds=inputs_embeds,\n","        )\n","\n","        pooled_output = outputs[1]\n","\n","        pooled_output = self.dropout(pooled_output)\n","        logits = self.classifier(pooled_output)\n","\n","        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n","\n","        if labels is not None:\n","            if self.num_labels == 1:\n","                #  We are doing regression\n","                loss_fct = MSELoss()\n","                loss = loss_fct(logits.view(-1), labels.view(-1))\n","            else:\n","                loss_fct = CrossEntropyLoss()\n","                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","            outputs = (loss,) + outputs\n","\n","        return outputs  # (loss), logits, (hidden_states), (attentions)"],"metadata":{"id":"-DabnSgjAFKR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train\n","\n","import gc\n","import os\n","\n","import numpy as np\n","from torch.utils.data import dataloader\n","from tqdm import tqdm\n","from transformers import AdamW\n","\n","# import KoBERTforSequenceClassfication\n","# import WellnessTextClassificationDataset\n","\n","\n","def train(device, epoch, model, optimizer, train_loader, save_step, save_ckpt_path, train_step=0):\n","    losses = []\n","    train_start_index = train_step + 1 if train_step != 0 else 0\n","    total_train_step = len(train_loader)\n","    model.train()\n","\n","    with tqdm(total=total_train_step, desc=f\"Train({epoch})\") as pbar:\n","        pbar.update(train_step)\n","        for i, data in enumerate(train_loader, train_start_index):\n","\n","            optimizer.zero_grad()\n","            outputs = model(**data)\n","\n","            loss = outputs[0]\n","\n","            losses.append(loss.item())\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            pbar.update(1)\n","            pbar.set_postfix_str(f\"Loss: {loss.item():.3f} ({np.mean(losses):.3f})\")\n","\n","            if i >= total_train_step or i % save_step == 0:\n","                torch.save({\n","                    'epoch': epoch,  # 현재 학습 epoch\n","                    'model_state_dict': model.state_dict(),  # 모델 저장\n","                    'optimizer_state_dict': optimizer.state_dict(),  # 옵티마이저 저장\n","                    'loss': loss.item(),  # Loss 저장\n","                    'train_step': i,  # 현재 진행한 학습\n","                    'total_train_step': len(train_loader)  # 현재 epoch에 학습할 총 train step\n","                }, save_ckpt_path)\n","\n","    return np.mean(losses)\n","\n","\n","if __name__ == '__main__':\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","    data_path = \"/gdrive/My Drive/KoBERT/data/wellness_dataset.txt\"\n","    checkpoint_path = \"/gdrive/My Drive/KoBERT/checkpoint\"\n","    save_ckpt_path = f\"{checkpoint_path}/wellness_dataset.pth\"\n","\n","    n_epoch = 50  # Num of Epoch\n","    batch_size = 8   # 배치 사이즈\n","    ctx = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    device = torch.device(ctx)\n","    save_step = 10  # 학습 저장 주기\n","    learning_rate = 5e-6  # Learning Rate\n","\n","    # WellnessTextClassificationDataset Data Loader\n","    dataset = WellnessTextClassificationDataset(file_path=data_path, device=device)\n","    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","    model = KoBERTforSequenceClassfication()\n","    model.to(device)\n","\n","    # Prepare optimizer and schedule (linear warmup and decay)\n","    no_decay = ['bias', 'LayerNorm.weight']\n","    optimizer_grouped_parameters = [\n","        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","    ]\n","    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n","\n","    pre_epoch, pre_loss, train_step = 0, 0, 0\n","    if os.path.isfile(save_ckpt_path):\n","        checkpoint = torch.load(save_ckpt_path, map_location=device)\n","        pre_epoch = checkpoint['epoch']\n","        train_step = checkpoint['train_step']\n","        total_train_step = checkpoint['total_train_step']\n","\n","        model.load_state_dict(checkpoint['model_state_dict'])\n","        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","\n","        print(f\"load pretrain from: {save_ckpt_path}, epoch={pre_epoch}\")\n","\n","    losses = []\n","    offset = pre_epoch\n","    for step in range(n_epoch):\n","        epoch = step + offset\n","        \n","        loss = train(device, epoch, model, optimizer, train_loader, save_step, save_ckpt_path, train_step)\n","        losses.append(loss)"],"metadata":{"id":"jXIS3qoZAF6y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# test\n","\n","import random\n","\n","\n","def load_emotion_category():\n","\n","    category_path=\"/gdrive/My Drive/KoBERT/data/emotion_category.txt\"\n","\n","    c_f = open(category_path, 'r')\n","\n","    category_lines = c_f.readlines()\n","\n","    category = {}\n","    for line_num, line_data in enumerate(category_lines):\n","        data = line_data.split('\\t')\n","        category[data[1][:-1]] = data[0]\n","\n","    return category\n","\n","\n","def kobert_input(tokenizer, str, device=None, max_seq_len=512):\n","    index_of_words = tokenizer.encode(str)\n","    token_type_ids = [0] * len(index_of_words)\n","    attention_mask = [1] * len(index_of_words)\n","\n","    # Padding Length\n","    padding_length = max_seq_len - len(index_of_words)\n","\n","    # Zero Padding\n","    index_of_words += [0] * padding_length\n","    token_type_ids += [0] * padding_length\n","    attention_mask += [0] * padding_length\n","\n","    data = {\n","        'input_ids': torch.tensor([index_of_words]).to(device),\n","        'token_type_ids': torch.tensor([token_type_ids]).to(device),\n","        'attention_mask': torch.tensor([attention_mask]).to(device),\n","    }\n","    return data\n","\n","\n","if __name__ == \"__main__\":\n","    checkpoint_path = \"/gdrive/My Drive/KoBERT/checkpoint\"\n","    save_ckpt_path = f\"{checkpoint_path}/wellness_dataset.pth\"\n","\n","    # 답변과 카테고리 불러오기\n","    category = load_emotion_category()\n","\n","    ctx = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    device = torch.device(ctx)\n","\n","    # 저장한 Checkpoint 불러오기\n","    checkpoint = torch.load(save_ckpt_path, map_location=device)\n","\n","    model = KoBERTforSequenceClassfication()\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","\n","    model.to(ctx)\n","    model.eval()\n","\n","    tokenizer = get_tokenizer()\n","\n","    while 1:\n","        sent = input('\\nQuestion: ')  # '요즘 기분이 우울한 느낌이에요'\n","        data = kobert_input(tokenizer, sent, device, 512)\n","\n","\n","        if '종료' in sent:\n","          \n","            break\n","\n","        output = model(**data)\n","\n","        logit = output[0]\n","        softmax_logit = torch.softmax(logit, dim=-1)\n","        softmax_logit = softmax_logit.squeeze()\n","\n","        max_index = torch.argmax(softmax_logit).item()\n","        max_index_value = softmax_logit[torch.argmax(softmax_logit)].item()\n","\n","        emotion = category[str(max_index)]\n","\n","        print(f'index: {max_index}, emotion: {emotion}, softmax_value: {max_index_value}')\n","        print('-' * 50)"],"metadata":{"id":"ZCP3wF_XAJY-"},"execution_count":null,"outputs":[]}]}